{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "This notebook contains the code for evaluating both response and retrieval quality for the RAG model. \n",
    "It includes the pipeline for evaluating all metrics, although not all metrics were calculated for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to root\n",
    "path_to_root = '/work/KlaraKrÃ¸yerFomsgaard#1926/NLP_2023_P'\n",
    "\n",
    "# To API key file\n",
    "path_to_key = f'{path_to_root}/config/keys.txt'\n",
    "\n",
    "# To data folder\n",
    "path_to_data = f'{path_to_root}/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "# -- Import custom functions from /src\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'{path_to_root}/src')\n",
    "\n",
    "from utils import extract_text_snippet\n",
    "from evaluate_models import faithfullness_eval, relevancy_eval, correctness_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import data\n",
    "\n",
    "df = pd.read_csv(f\"{path_to_data}/results/file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-specific evaluation\n",
    "This section includes evaluation metrics that are specific to a RAG setup. It includes\n",
    "1. Guideline adherence\n",
    "2. Context retrieval\n",
    "3. Faithfulness\n",
    "4. Relevancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. Guideline adherence*\n",
    "\n",
    "The following section containg code for evaluation of guideline adherence. \n",
    "The RAG is evaluated on the following:\n",
    "\n",
    "1. Does it start the response with the prefix \"*Based on the context...*\" or \"*Based on prior knowledge...*\"\n",
    "2. Does it provide a reference list with working links in the response?\n",
    "3. Does one of the links in the response correspond to the link in the context metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PREFIX\n",
    "\n",
    "# --- Create new column for storing evaluation metric\n",
    "df['prefix_correct'] = ''\n",
    "\n",
    "# --- Check if the prefix is found in the response - if found, give a 'pass'\n",
    "for i in range(len(df)):\n",
    "    true_context = pd.Series(df['LLMRAG_response'][i])\n",
    "    if true_context.str.contains('Based on', regex=False).any(): #true_context.str.contains('Based on prior knowledge', regex=False).any() or \n",
    "        df['prefix_correct'][i] = 'Pass'\n",
    "    else:\n",
    "        df['prefix_correct'][i] = 'Fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254711/471175158.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['url_correct'][i] = 'No reference'\n",
      "/tmp/ipykernel_254711/471175158.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['url_active'][i] = 'No reference'\n",
      "/tmp/ipykernel_254711/471175158.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['url_response'][i] = url_response\n",
      "/tmp/ipykernel_254711/471175158.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['url_response'][i] = [url.replace('>', '') for url in df['url_response'][i]]\n",
      "/tmp/ipykernel_254711/471175158.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['url_active'][i] = active_links\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.com/news/world-middle-east-57885555\n",
      "https://www.aljazeera.com/news/2021/10/12/moderate-muslims-seek-to-separate-religion-from-extremism\n",
      "https://www.theguardian.com/world/2021/oct/12/moderate-muslims-seek-to-separate-religion-from-extremism\n",
      "https://news.mit.edu/2015/algorithms-recognize-objects-f\n"
     ]
    }
   ],
   "source": [
    "# -- LINKS\n",
    "\n",
    "# --- Function for checking if URL is active\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def is_url_active(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10, verify=True)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# --- Create columns for storing extracted URLs and evaluation metrics\n",
    "df['url_true_context'] = '' # The URL from the metadata of the 'true' context\n",
    "df['url_response'] = '' # URLs included in the RAG response\n",
    "df['url_correct'] = '' # Column for evaluation metric \n",
    "\n",
    "# --- Create column for evaluation of working links\n",
    "df['url_active'] = ''\n",
    "\n",
    "# --- Mark unwanted symbols in URLs for comparison\n",
    "bad_chars_url = [']','}','{','[',\"'\",\"\\\\\"] \n",
    "\n",
    "# --- Check if working link in response, and if it matches with true context link \n",
    "for i in range(len(df)):\n",
    "    #url_true_context = re.findall(r'https?://[^\\s\\'\"}]+',df['metadata'][i])\n",
    "    url_response = re.findall(r'https?://[^\\s\\'\"}]+',df['LLMRAG_response'][i])\n",
    "\n",
    "    # If the response do not have a reference\n",
    "    if url_response == []:\n",
    "        df['url_correct'][i] = 'No reference'\n",
    "        df['url_active'][i] = 'No reference'\n",
    "\n",
    "        #Remove unwanted symbols from string\n",
    "        #for k in bad_chars_url:\n",
    "            #df['url_true_context'][i] = url_true_context[0].replace(k,'') \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Remove unwanted symbols from string\n",
    "        for k in bad_chars_url:\n",
    "            #df['url_true_context'][i] = url_true_context[0].replace(k,'')\n",
    "            \n",
    "            for j in url_response:\n",
    "                j = j.replace(k,'')\n",
    "\n",
    "            df['url_response'][i] = url_response\n",
    "            break\n",
    "\n",
    "        # Additional cleaning\n",
    "        df['url_response'][i] = [url.replace('>', '') for url in df['url_response'][i]]\n",
    "        \n",
    "        # Check if the context link can be found in the links provided as references\n",
    "        #if df['url_true_context'][i] in df['url_response'][i]:\n",
    "            #df['url_correct'][i] = 'Pass'\n",
    "        #else:\n",
    "            #df['url_correct'][i] = 'Fail'\n",
    "        \n",
    "        # Check if links are active - provide a list of true/false\n",
    "        active_links = []\n",
    "        for url in df['url_response'][i]:\n",
    "            if is_url_active(url):\n",
    "                active_links.append('True')\n",
    "            else:\n",
    "                active_links.append('False')\n",
    "                print(url)\n",
    "        \n",
    "        df['url_active'][i] = active_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2. Context retrieval*\n",
    "\n",
    "The following section containg code for evaluation of guideline adherence.\n",
    "The RAG is evaluated on whether it retrieves the 'true context', i.e. the context used for query generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retrieval_correct'] = ''\n",
    "\n",
    "# Mark unwanted special characters\n",
    "bad_chars = [']', '[', \"'\",\"\\\\\"] \n",
    "\n",
    "for i in range(len(df)):\n",
    "    true_context = pd.Series(df['text'].iloc[i])\n",
    "\n",
    "    context_labels = []\n",
    "\n",
    "    for j in df.columns[12:15]:\n",
    "        context = extract_text_snippet(df[j].iloc[i])\n",
    "\n",
    "        # initializing bad_chars_list\n",
    "        for k in bad_chars:\n",
    "            true_context[0] = true_context[0].replace(k,'')\n",
    "            context = context.replace(k, '')\n",
    "        \n",
    "        context_label = []\n",
    "        context_bool = true_context.str.contains(context, regex=False)\n",
    "        context_label.append(context_bool[0])\n",
    "        context_labels.append(context_label[0])\n",
    "    \n",
    "    #print(context_labels)\n",
    "\n",
    "    if True in context_labels:\n",
    "        df['retrieval_correct'][i] = 'Pass'\n",
    "    else:\n",
    "        df['retrieval_correct'][i] = 'Fail'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *3. Faithfulness*\n",
    "The following used GPT-3.5 Turbo to evaluate the faithfullness of the response to the retrieved nodes. \n",
    "The full nodes are matched with the node_columns (which have been cut off in the conversion to df) and appended to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    OpenAIEmbedding,\n",
    "    PromptHelper,\n",
    "    )\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "# Function to retrieve nodes from dataset\n",
    "def get_nodes(dataset):\n",
    "    # Convert the DataFrame into a list of Document objects that the index can understand\n",
    "    documents = [Document(text=row['Article Body'],\n",
    "                        metadata={'title': row['Article Header'],\n",
    "                                    'source': row['Source'],\n",
    "                                    'author': row['Author'],\n",
    "                                    'date': row['Published Date'],\n",
    "                                    'url': row['Url']}) for index, row in dataset.iterrows()] \n",
    "\n",
    "    # --- Sentencesplitter to split into chunks\n",
    "    text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "    nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get nodes from dataset\n",
    "AI_data = pd.read_csv(f\"{path_to_data}/articles_full_aug.csv\")\n",
    "\n",
    "nodes = get_nodes(dataset = AI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert df to long format with necessary columns for evaluation and create new column for the full node\n",
    "df_long = pd.melt(df, \n",
    "                id_vars=['text','gold_response','query','LLMRAG_response','LLM_response'], # add 'text' column if not G_factoid and 'gold_response' if factoid\n",
    "                value_vars=['node_0','node_1','node_2']) # Add k nodes\n",
    "\n",
    "df_long['full_node'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Append the full node corresponding to the retrieved contexts\n",
    "for i in range(len(df_long)):\n",
    "    context = extract_text_snippet(df_long['value'][i])\n",
    "    \n",
    "    for j in range(len(nodes)):\n",
    "        if context in nodes[j].text:\n",
    "            df_long['full_node'][i] = nodes[j].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FAITHFULLNESS\n",
    "# --- OBS: Requires OpenAI API\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Create columns for storing correctness\n",
    "df_long['faithfullness_bin'] = ''\n",
    "df_long['faithfullness_score'] = ''\n",
    "df_long['faithfullness_feedback'] = ''\n",
    "\n",
    "# --- Run evaluation\n",
    "for i in range(len(df_long)):\n",
    "    result = faithfullness_eval(response_str= df_long['LLMRAG_response'][i],retrieved_nodes = [df_long['full_node'][i]]) #full_node\n",
    "    df_long['faithfullness_bin'][i] = result[0]\n",
    "    df_long['faithfullness_score'][i] = result[1]\n",
    "    df_long['faithfullness_feedback'][i] = result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4. Relevancy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- RELEVANCY\n",
    "# --- OBS: Requires OpenAI API\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Create columns for storing correctness\n",
    "df_long['relevancy_bin'] = ''\n",
    "df_long['relevancy_score'] = ''\n",
    "df_long['relevancy_feedback'] = ''\n",
    "\n",
    "# --- Run evaluation\n",
    "for i in range(len(df_long)):\n",
    "    result = relevancy_eval(query_str= df_long['query'][i],response_str= df_long['LLMRAG_response'][i],retrieved_nodes = [df_long['full_node'][i]])\n",
    "    df_long['relevancy_bin'][i] = result[0]\n",
    "    df_long['relevancy_score'][i] = result[1]\n",
    "    df_long['relevancy_feedback'][i] = result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response quality: Fact retrieval\n",
    "The following section includes evaluation of correctness for both RAG and ordinary LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Create columns for storing correctness\n",
    "df['correctness_RAG_bin'] = ''\n",
    "df['correctness_RAG_score'] = ''\n",
    "df['correctness_RAG_feedback'] = ''\n",
    "df['correctness_LLM_bin'] = ''\n",
    "df['correctness_LLM_score'] = ''\n",
    "df['correctness_LLM_feedback'] = ''\n",
    "\n",
    "# --- Run evaluation\n",
    "for i in range(len(df)):\n",
    "    result_RAG = correctness_eval(query_str= df['query'][i],response_str= df['LLMRAG_response'][i],reference_str=df['gold_response'][i])\n",
    "    df['correctness_RAG_bin'][i] = result_RAG[0]\n",
    "    df['correctness_RAG_score'][i] = result_RAG[1]\n",
    "    df['correctness_RAG_feedback'][i] = result_RAG[2]\n",
    "\n",
    "    result_LLM = correctness_eval(query_str= df['query'][i],response_str= df['LLM_response'][i],reference_str=df['gold_response'][i])\n",
    "    df['correctness_LLM_bin'][i] = result_LLM[0]\n",
    "    df['correctness_LLM_score'][i] = result_LLM[1]\n",
    "    df['correctness_LLM_feedback'][i] = result_LLM[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Write df & df_long to .csv\n",
    "\n",
    "df.to_csv(f\"{path_to_data}/results/the_dataset_name_evaluation.csv\")\n",
    "df_long.to_csv(f\"{path_to_data}/results/the_dataset_name_evaluation_long.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate hit rate (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.read_csv(f\"{path_to_data}/results/G_factoid_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics_df = pd.DataFrame()\n",
    "\n",
    "acc_ref = df_sf['correct_reference'].value_counts('Pass')[0]\n",
    "acc_node_retrieval = df_sf['retrieval_correct'].value_counts('Pass')[0]\n",
    "acc_guide_link = 1 - df_sf['correct_reference'].value_counts('Pass')[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exam",
   "language": "python",
   "name": "nlp-exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
